Um algoritmo e uma sequencia finita de instrucoes bem definidas e nao ambiguas, cada uma das quais pode ser executada mecanicamente num periodo de tempo finito e com uma quantidade de esforco finita.
O conceito de algoritmo e frequentemente ilustrado pelo exemplo de uma receita culinaria, embora muitos algoritmos sejam mais complexos. Eles podem repetir passos (fazer iteracoes) ou necessitar de decisoes (tais como comparacoes ou logica) ate que a tarefa se complete. Um algoritmo corretamente executado nao ira resolver um problema se estiver implementado incorretamente ou se nao for apropriado ao problema.
Um algoritmo nao representa, necessariamente, um programa de computador , e sim os passos necessarios para realizar uma tarefa. Sua implementacao pode ser feita por um computador, por outro tipo de automato ou mesmo por um ser humano. Diferentes algoritmos podem realizar a mesma tarefa usando um conjunto diferenciado de instrucoes em mais ou menos tempo, espaco ou esforco do que outros.
Tal diferenca pode ser reflexo da complexidade computacional aplicada, que depende de estruturas de dados adequadas ao algoritmo. Por exemplo, um algoritmo para se vestir pode especificar que voce vista primeiro as meias e os sapatos antes de vestir a calca enquanto outro algoritmo especifica que voce deve primeiro vestir a calca e depois as meias e os sapatos. Fica claro que o primeiro algoritmo e mais dificil de executar que o segundo apesar de ambos levarem ao mesmo resultado.
O conceito de um algoritmo foi formalizado em 1936 pela Maquina de Turing de Alan Turing e pelo calculo lambdak de Alonzo Church, que formaram as primeiras fundacoes da Ciencia da computacao.
